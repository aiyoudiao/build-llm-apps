#  day4-1

## Embeddings和向量数据库技术综述

## 一、Embeddings和向量数据库

### 1. Embedding

- （1）嵌入的定义

	- 本质：将高维数据压缩到固定低维空间的过程
	- 作用：通过体积压缩实现高效存储和计算
	- 应用场景：信息存储管理、向量相似度计算（如余弦相似度）

- （2）Word Embedding

	- 核心功能：将单词转化为向量表示
	- 相似度衡量：使用余弦相似度
	- 模型参数：

		- 向量维度（1024/2048等）
		- 模型大小
		- 性能影响：维度越高计算越慢

- （3）传统语义特征提取方法

	- 数据集和目标

		- 数据集：西雅图152家酒店数据（名称/地址/描述）
		- 目标：基于文本相似度推荐Top10相似酒店

	- 特征提取

		- 分词：使用结巴分词器
		- 句子示例：

			- &quot;这个程序代码太乱那个代码规范&quot; → [&quot;这个&quot;,&quot;程序&quot;,&quot;代码&quot;,&quot;太乱&quot;,&quot;那个&quot;,&quot;代码&quot;,&quot;规范&quot;]
			- &quot;这个程序代码不规范那个更规范&quot; → [&quot;这个&quot;,&quot;程序&quot;,&quot;代码&quot;,&quot;不&quot;,&quot;规范&quot;,&quot;那个&quot;,&quot;更&quot;,&quot;规范&quot;]

		- 特征构建：

			- 统计所有句子中的唯一单词（特征维度）
			- 计算每句中各单词频次
			- 向量表示示例：

				- 句子A：[1,1,2,1,1,1,0,0]
				- 句子B：[1,1,1,0,1,2,1,1]

		- 余弦相似度：

			- 计算公式：
[
\cos \theta = \frac{A \cdot B}{|A||B|}
]
			- 取值范围：[-1,1]（1表示完全一致）
			- 示例：结果0.738，越接近1越相似

		- 存在的问题：

			- 忽略词序影响
			- 反例：调整语序后仅向量不变，语义却相反

		- 改进方案

			- N元语法（二元、三元等）引入
			- 能捕捉词序差异，如&quot;不规范&quot;与&quot;规范不&quot;

- （4）应用案例

	- 数据处理

		- 使用西雅图酒店数据，分析描述文本的n-gram特征
		- 特征工程：

			- 一元、二元、三元短语
			- TF-IDF：结合词频和逆文档频率
			- 特征向量：26,000+维

	- 相似度计算

		- 利用余弦相似度
		- 构建152×152的相似性矩阵
		- 推荐逻辑：为目标酒店按相似度排序返回Top10

	- 系统实现

		- 步骤：

			- 数据清洗（小写、去停用词）
			- 特征提取（TF-IDF+N-gram）
			- 计算余弦相似度
			- 返回Top10

		- 代码关键点：

			- CountVectorizer和TfidfVectorizer
			- linear_kernel
			- pandas.Series排序

- （5）内容推荐系统

	- 特征提取：

		- 使用clean_text函数
		- 提取1-3元语法特征
		- 生成稀疏向量

	- 相似度：

		- 使用线性核（余弦）
		- 结果值范围：0-1

	- 推荐算法流程：

		- 根据名称索引
		- 获取相似度向量
		- 排除自身
		- 返回Top10

	- 优化思考：

		- 高维稀疏问题
		- 维度降低（如300维）提升效率

- （6）Word Embedding总结

	- 内容推荐

		- 提取：

			- N-Gram（连续字）
			- TF-IDF关键词

		- 计算：

			- 余弦相似度
			- 选择Top-K

	- 降维作用

		- 将高维稀疏特征压缩到低维（1024/128维）
		- 计算便捷

	- 通用性

		- 万物向量化
		- 余弦相似度
		- 例：人物性格、财富等

	- 谷歌Word2Vec

		- 训练：大规模语料，生成固定维度向量
		- 向量运算：king - man + woman ≈ queen
		- 特性：解决one-hot爆炸问题，保持相似性
		- 模式：

			- Skip-Gram
			- CBOW

		- 工具参数：size、window、min_count
		- 例：西游记人物相似度
		- 价值：

			- 语义表示
			- 向量运算

### 7. Word2Vec实战

- 训练流程：文本→分词→训练模型
- 关系验证：相似词、向量操作
- 主要工具：jieba、Gensim

### 8. Embedding总结

- 作用：

	- 降维
	- 语义关系

- 应用：

	- 推荐、搜索、知识库

- 例：曹操+刘备-张飞

## 二、知识讲解

### 1. 例题：三国演义嵌入

- 目标：训练词向量
- 任务：

	- 找与&quot;曹操&quot;最相近词
	- 向量运算：曹操+刘备-张飞=？

- 数据：三国演义文本
- 原理：低维空间保持语义

### 2. Word2Vec工具使用

- （1）基本介绍

	- 功能：词向量转换
	- 计算：余弦相似度
	- 参数：

		- size
		- window
		- min_count

	- 应用：词相似、向量距离

- （2）压缩原理

	- 从高维稀疏到低维稠密
	- 常用：128、256维
	- 模型：Word2Rect/BERT

- （3）模型选择

	- 简单任务：Word2Rect
	- 复杂任务：BERT
	- 原理：特征提取器

### 3. Embedding作用

- 表示：连续向量
- 计算：相似度
- 优势：解决维度爆炸
- 示例：king- man+woman≈queen

### 4. 跨模态与多语言

- 图像向量：ResNet、CLIP
- 多语言：多语模型（paraphrase-multilingual）
- 作用：跨模态融合、多语言理解

### 5. Embedding模型与LLM

- 差异：

	- 任务：理解vs生成
	- 成本：低（Embedding）vs高（LLM）
	- 效率：Embedding快

- 作用：

	- Embedding：检索
	- LLM：内容生成

### 6. 向量性能影响

- 维度影响：

	- 高维：丰富语义（1024/4096）
	- 低维：快（256/512）

- 套娃模型：先高维生成，再截取（Matryoshka）
- 依据：

	- 任务需求
	- 实时性

### 7. 语义搜索应用

- 搜索：私有知识库
- 推荐：基于相似度
- 典型：FAISS、Milvus、Pinecone

### 8. 量库与架构

- 主要工具：

	- Faiss：高性能
	- Milvus：分布式
	- Pinecone：云服务
	- Weaviate：自动向量化
	- Qdrant：高性能过滤

- 元数据存储：

	- Redis
	- PostgreSQL
	- MongoDB

- 核心：

	- 存向量+元数据
	- 近似匹配
	- 支持多模态

- 实践：

	- 文档准备→向量化→存储→检索

## 三、课程总结

### 向量数据库：存储管理大量向量，辅助大模型

### 实践要点：

- 选择合适模型
- 管理元数据
- 验证效果

### 核心点：

- 降维、相似度
- 结合业务需求

### 学习建议：

- 聚焦酒店案例
- 理解整体流程
- 逐步深入技术细节

## 四、知识小结

### Embedding：高维转低维、保留语义

### 特征提取：N元、TF-IDF

### 相似度：余弦

### 词向量：Word2Vec

### 向量数据库：

- Faiss、Milvus、Pinecone
- 支持大规模高效检索

### 量库应用：

- 检索、推荐、知识库

### 未来方向：

- 跨模态、多语言
- 模型辅助大模型

