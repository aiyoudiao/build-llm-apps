## Day 4 总结

主要是**向量（Embedding）**在 NLP 中的应用，从最传统的 TF-IDF 推荐，到 Word2Vec 词向量训练，再到现代的 FAISS 向量数据库应用。核心逻辑是：**万物皆可向量化**，然后通过计算向量之间的距离（相似度）来解决推荐、搜索和分类问题。

### 记忆卡片

- **酒店推荐（TF-IDF）**：用 TF-IDF 提取酒店描述的关键词特征，算余弦相似度（游戏中检测视野），找出描述最像的酒店。
- **词向量（Word2Vec）**：用 `gensim` 训练《西游记》和《三国演义》，学会了 `model.wv.similarity` 算词义相似度，比如“孙悟空”和“孙行者”很近。
- **向量数据库（FAISS 基础）**：用 `IndexFlatL2` 做暴力搜索，把文档变成向量存进去，再查“退货政策”这种语义相关的文档。
- **向量数据库（FAISS 进阶）**：针对 10 亿级数据，用 `IVF-PQ`（倒排+量化）索引，配合 `mmap` 内存映射，实现海量数据的毫秒级查询和低内存占用。

### 理解

- **传统 NLP 部分**：

  - `1.酒店推荐/hotel_rec.py` 展示了在没有深度学习之前，怎么用统计学方法（TF-IDF）做推荐。核心是“词频、区分度”代表特征。
- **Word2Vec 部分**：

  - `2.单词转向量/` 下的代码展示了如何把文本变成计算机能理解的数字。
  - 核心流程：分词（jieba）→ 训练（Word2Vec）→ 推理（most_similar）。
  - 有趣的发现：模型能学会“孙悟空”和“猪八戒”是相关的，甚至能做类比推理。
- **向量数据库部分**：

  - `3.向量数据库/` 是今天的重头戏。
  - **基础流程**：OpenAI/DashScope 算 Embedding → 存入 FAISS → 搜索最相似向量。
  - **工程化思考**：
    - 小数据（<100万）：直接用 `IndexFlatL2`，精度高，存内存。
    - 大数据（10亿+）：必须用 `IVF`（倒排，减少搜索范围）+ `PQ`（量化，压缩向量体积），否则内存存不下，搜索慢死。
    - **持久化**：索引存 `.bin`，元数据存 `.pkl`（pickle），生产环境必备。
- “怎么做简单的文本推荐” → `1.酒店推荐/hotel_rec.py`
- “怎么训练自己的词向量” → `2.单词转向量/a.西游记/word_similarity.py`
- “怎么用向量数据库做语义搜索” → `3.向量数据库/4-query-from-faiss-本地库.py`
- “海量数据怎么存向量库” → `3.向量数据库/5-save-to-faiss-模拟大数据.py`
